# -*- coding: utf-8 -*-
"""Copy of granite-speech-3.3-8b.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kqUUKvr5oK0D5HI7epnEji9Qa0k17ep5
"""

!pip install -U transformers

"""## Local Inference on GPU
Model page: https://huggingface.co/ibm-granite/granite-speech-3.3-8b

‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/ibm-granite/granite-speech-3.3-8b)
			and/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè
"""

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("automatic-speech-recognition", model="ibm-granite/granite-speech-3.3-8b")

# Load model directly
from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq

processor = AutoProcessor.from_pretrained("ibm-granite/granite-speech-3.3-8b")
model = AutoModelForSpeechSeq2Seq.from_pretrained("ibm-granite/granite-speech-3.3-8b")

!pip install PyMuPDF transformers

import fitz  # part of PyMuPDF
from transformers import pipeline

# Load classification model
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

import fitz  # from PyMuPDF
import requests

def extract_text_from_pdf(pdf_path):
    """Extract text from all pages in a PDF file using PyMuPDF (fitz)."""
    text = ''
    try:
        doc = fitz.open(pdf_path)
        for page in doc:
            text += page.get_text()
    except Exception as e:
        print(f"Error reading PDF: {e}")
    return text

def classify_sentences(text):
    """
    Simulate Watsonx classification.
    Replace the request URL and payload with actual IBM Watsonx endpoint and headers if available.
    """
    # Dummy Watsonx API simulation ‚Äî replace this with your actual API
    watsonx_url = "https://watsonx.ibm.com/api/classify"  # Change this to actual Watsonx endpoint
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer YOUR_API_KEY"  # Add real key if required
    }

    sentences = [s.strip() for s in text.split('.') if s]
    classified = {}

    for sentence in sentences:
        try:
            response = requests.post(
                watsonx_url,
                json={"text": sentence},
                headers=headers
            )
            response.raise_for_status()
            data = response.json()
            phase = data.get('phase', 'Uncategorized')
        except Exception as e:
            print(f"Failed to classify sentence: '{sentence}' ‚Äî {e}")
            phase = 'Uncategorized'

        classified.setdefault(phase, []).append(sentence)

    return classified

# Example usage
if __name__ == "__main__":
    pdf_path = "requirements.pdf"  # Ensure this file exists in the same directory
    pdf_text = extract_text_from_pdf(pdf_path)
    if pdf_text:
        classified_requirements = classify_sentences(pdf_text)
        print(classified_requirements)
    else:
        print("No text found in PDF.")

pip install transformers accelerate

pip install torch --index-url https://download.pytorch.org/whl/cu118

from transformers import pipeline

# No login or token required
generator = pipeline("text-generation", model="tiiuae/falcon-rw-1b")

prompt = "Write a Python function to sort a list of numbers using bubble sort"

response = generator(prompt, max_new_tokens=150)
print(response[0]["generated_text"])

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

# Use a freely available model like Falcon (or choose another open-access model)
model_name = "tiiuae/falcon-rw-1b"

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Create text-generation pipeline
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Buggy Python code
buggy_code = """
def add_numbers(a, b)
    return a + b
"""

# Instruction to fix the bug
instruction = "Fix the syntax error in the following Python code:\n" + buggy_code
formatted_prompt = instruction  # Falcon does not require special formatting like <s>[INST] ... [/INST]>

# Generate model response
response = generator(formatted_prompt, max_new_tokens=100)
print(response[0]['generated_text'])

prompt = "Write unit tests for a function that checks if a number is prime in Python"

response = generator(f"<s>[INST] {prompt} [/INST]", max_new_tokens=150)
print(response[0]["generated_text"])

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

code_expl = """
def factorial(n):
    if n == 0:
        return 1
    else:
        return n * factorial(n-1)
"""

summary = summarizer(f"This code defines a recursive factorial function:\n{code_expl}", max_length=50, min_length=10, do_sample=False)
print("üß† Summary:", summary[0]["summary_text"])

from transformers import pipeline

qa = pipeline("question-answering", model="distilbert-base-cased-distilled-squad")

context = """
Software Development Life Cycle (SDLC) is a structured process followed by software engineers to develop software. The phases include Requirement Analysis, Design, Development, Testing, Deployment, and Maintenance. Each phase has specific deliverables.
"""

question = "What is requirement analysis in SDLC?"

output = qa(question=question, context=context)
print("üß† Answer:", output["answer"])